# 第11章：文件读写

Pandas支持多种文件格式的读写，是数据处理的重要环节。本章将详细介绍如何使用Pandas读写CSV、Excel、JSON、SQL、HDF5等多种格式的文件。

## 11.1 CSV文件

### 11.1.1 读取CSV文件

```python
import pandas as pd
import numpy as np
from io import StringIO

# 创建示例CSV数据
csv_data = """Name,Age,City,Salary
Alice,25,Beijing,5000
Bob,30,Shanghai,6000
Charlie,35,Guangzhou,7000
David,28,Shenzhen,5500"""

# 从字符串读取
df = pd.read_csv(StringIO(csv_data))
print("读取CSV数据：")
print(df)

# 常用参数
# df = pd.read_csv('data.csv',
#                  sep=',',              # 分隔符
#                  header=0,             # 标题行
#                  names=None,           # 列名
#                  index_col=None,       # 索引列
#                  usecols=None,         # 选择的列
#                  dtype=None,           # 数据类型
#                  skiprows=None,        # 跳过的行
#                  nrows=None,           # 读取的行数
#                  encoding='utf-8',     # 编码
#                  parse_dates=False)    # 解析日期
```

### 11.1.2 读取CSV的高级选项

```python
# 示例1：指定分隔符
tsv_data = """Name\tAge\tCity
Alice\t25\tBeijing
Bob\t30\tShanghai"""

df_tsv = pd.read_csv(StringIO(tsv_data), sep='\t')
print("\n读取TSV数据：")
print(df_tsv)

# 示例2：跳过行
csv_with_comments = """# 这是注释行
# 数据开始
Name,Age,City
Alice,25,Beijing
Bob,30,Shanghai"""

df_skip = pd.read_csv(StringIO(csv_with_comments), skiprows=2)
print("\n跳过注释行：")
print(df_skip)

# 示例3：指定列名
csv_no_header = """Alice,25,Beijing
Bob,30,Shanghai"""

df_no_header = pd.read_csv(StringIO(csv_no_header),
                           header=None,
                           names=['Name', 'Age', 'City'])
print("\n指定列名：")
print(df_no_header)

# 示例4：选择特定列
csv_many_cols = """Name,Age,City,Salary,Department
Alice,25,Beijing,5000,IT
Bob,30,Shanghai,6000,Sales"""

df_selected = pd.read_csv(StringIO(csv_many_cols),
                         usecols=['Name', 'Age', 'Salary'])
print("\n选择特定列：")
print(df_selected)

# 示例5：指定数据类型
csv_types = """Name,Age,Score
Alice,25,85.5
Bob,30,92.0
Charlie,35,88.5"""

df_types = pd.read_csv(StringIO(csv_types),
                       dtype={'Name': str, 'Age': int, 'Score': float})
print("\n指定数据类型：")
print(df_types.dtypes)

# 示例6：解析日期
csv_dates = """Name,Date,Value
Alice,2024-01-01,100
Bob,2024-01-02,150
Charlie,2024-01-03,120"""

df_dates = pd.read_csv(StringIO(csv_dates),
                       parse_dates=['Date'])
print("\n解析日期：")
print(df_dates)
print(df_dates.dtypes)
```

### 11.1.3 处理缺失值

```python
# 包含缺失值的CSV
csv_missing = """Name,Age,City,Salary
Alice,25,Beijing,5000
Bob,,Shanghai,6000
Charlie,35,,7000
David,28,Shenzhen,"""

df_missing = pd.read_csv(StringIO(csv_missing))
print("包含缺失值的数据：")
print(df_missing)

# 指定缺失值标记
csv_na = """Name,Age,City,Salary
Alice,25,Beijing,5000
Bob,NA,Shanghai,6000
Charlie,35,N/A,7000
David,28,Shenzhen,null"""

df_na = pd.read_csv(StringIO(csv_na),
                    na_values=['NA', 'N/A', 'null'])
print("\n指定缺失值标记：")
print(df_na)
```

### 11.1.4 写入CSV文件

```python
# 创建数据
df = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Charlie'],
    'Age': [25, 30, 35],
    'City': ['Beijing', 'Shanghai', 'Guangzhou'],
    'Salary': [5000, 6000, 7000]
})

# 基本写入
# df.to_csv('output.csv', index=False)
print("CSV写入示例（显示内容）：")
print(df.to_csv(index=False))

# 指定分隔符
print("\n使用制表符分隔：")
print(df.to_csv(sep='\t', index=False))

# 选择列
print("\n只写入部分列：")
print(df.to_csv(columns=['Name', 'Age'], index=False))

# 追加模式
# df.to_csv('output.csv', mode='a', header=False, index=False)

# 指定编码
# df.to_csv('output.csv', encoding='utf-8', index=False)
```

## 11.2 Excel文件

### 11.2.1 读取Excel文件

```python
# 读取Excel文件需要安装openpyxl或xlrd
# pip install openpyxl

# 创建示例Excel数据
df_excel = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Charlie'],
    'Age': [25, 30, 35],
    'City': ['Beijing', 'Shanghai', 'Guangzhou']
})

# 写入Excel（用于后续读取示例）
# df_excel.to_excel('sample.xlsx', index=False)

# 读取Excel文件
# df = pd.read_excel('sample.xlsx')
print("Excel读取示例：")
print(df_excel)

# 读取特定工作表
# df = pd.read_excel('sample.xlsx', sheet_name='Sheet1')

# 读取多个工作表
# dfs = pd.read_excel('sample.xlsx', sheet_name=['Sheet1', 'Sheet2'])
# dfs = pd.read_excel('sample.xlsx', sheet_name=None)  # 读取所有工作表
```

### 11.2.2 写入Excel文件

```python
# 创建数据
df1 = pd.DataFrame({
    'A': [1, 2, 3],
    'B': [4, 5, 6]
})

df2 = pd.DataFrame({
    'C': [7, 8, 9],
    'D': [10, 11, 12]
})

# 写入单个工作表
# df1.to_excel('output.xlsx', index=False)

# 写入多个工作表
# with pd.ExcelWriter('output.xlsx') as writer:
#     df1.to_excel(writer, sheet_name='Sheet1', index=False)
#     df2.to_excel(writer, sheet_name='Sheet2', index=False)

print("Excel写入示例：")
print("df1:")
print(df1)
print("\ndf2:")
print(df2)

# 格式化Excel输出
# with pd.ExcelWriter('formatted.xlsx', engine='openpyxl') as writer:
#     df1.to_excel(writer, sheet_name='Data', index=False)
#     workbook = writer.book
#     worksheet = writer.sheets['Data']
#     # 可以进一步设置格式
```

## 11.3 JSON文件

### 11.3.1 读取JSON文件

```python
# JSON字符串
json_str = """
[
    {"Name": "Alice", "Age": 25, "City": "Beijing"},
    {"Name": "Bob", "Age": 30, "City": "Shanghai"},
    {"Name": "Charlie", "Age": 35, "City": "Guangzhou"}
]
"""

# 从字符串读取
df_json = pd.read_json(json_str)
print("从JSON读取：")
print(df_json)

# 不同的JSON格式
# orient参数：'split', 'records', 'index', 'columns', 'values'

# records格式（默认）
json_records = """
[
    {"Name": "Alice", "Age": 25},
    {"Name": "Bob", "Age": 30}
]
"""
df1 = pd.read_json(json_records, orient='records')
print("\nrecords格式：")
print(df1)

# index格式
json_index = """
{
    "0": {"Name": "Alice", "Age": 25},
    "1": {"Name": "Bob", "Age": 30}
}
"""
df2 = pd.read_json(json_index, orient='index')
print("\nindex格式：")
print(df2)
```

### 11.3.2 写入JSON文件

```python
# 创建数据
df = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Charlie'],
    'Age': [25, 30, 35],
    'City': ['Beijing', 'Shanghai', 'Guangzhou']
})

# 不同格式的JSON输出
print("records格式：")
print(df.to_json(orient='records', indent=2))

print("\nindex格式：")
print(df.to_json(orient='index', indent=2))

print("\ncolumns格式：")
print(df.to_json(orient='columns', indent=2))

print("\nvalues格式：")
print(df.to_json(orient='values', indent=2))

print("\nsplit格式：")
print(df.to_json(orient='split', indent=2))

# 写入文件
# df.to_json('output.json', orient='records', indent=2)
```

## 11.4 SQL数据库

### 11.4.1 使用SQLite

```python
import sqlite3

# 创建内存数据库
conn = sqlite3.connect(':memory:')

# 创建示例数据
df = pd.DataFrame({
    'Name': ['Alice', 'Bob', 'Charlie'],
    'Age': [25, 30, 35],
    'City': ['Beijing', 'Shanghai', 'Guangzhou'],
    'Salary': [5000, 6000, 7000]
})

# 写入SQL数据库
df.to_sql('employees', conn, if_exists='replace', index=False)
print("数据已写入SQLite数据库")

# 从SQL读取
df_sql = pd.read_sql('SELECT * FROM employees', conn)
print("\n从SQL读取：")
print(df_sql)

# 带条件的查询
df_filtered = pd.read_sql('SELECT * FROM employees WHERE Age > 25', conn)
print("\n条件查询：")
print(df_filtered)

# 使用read_sql_query
df_query = pd.read_sql_query('SELECT Name, Salary FROM employees ORDER BY Salary DESC', conn)
print("\n查询结果：")
print(df_query)

conn.close()
```

### 11.4.2 使用其他数据库

```python
# MySQL示例（需要安装pymysql或mysqlclient）
# from sqlalchemy import create_engine
#
# engine = create_engine('mysql+pymysql://user:password@host:port/database')
# df = pd.read_sql('SELECT * FROM table_name', engine)
# df.to_sql('table_name', engine, if_exists='replace', index=False)

# PostgreSQL示例（需要安装psycopg2）
# engine = create_engine('postgresql://user:password@host:port/database')
# df = pd.read_sql('SELECT * FROM table_name', engine)

print("其他数据库连接示例（需要相应的驱动）")
```

## 11.5 HDF5文件

```python
# HDF5是一种高性能的二进制格式
# 适合存储大型数组和DataFrame

# 创建数据
df = pd.DataFrame({
    'A': np.random.randn(1000),
    'B': np.random.randn(1000),
    'C': np.random.randn(1000)
})

# 写入HDF5（需要安装tables: pip install tables）
# df.to_hdf('data.h5', key='df', mode='w')
print("HDF5文件操作示例")

# 读取HDF5
# df_h5 = pd.read_hdf('data.h5', 'df')

# 追加数据
# df.to_hdf('data.h5', key='df2', mode='a')

# 查询
# df_filtered = pd.read_hdf('data.h5', 'df', where='A > 0')
```

## 11.6 Parquet文件

```python
# Parquet是一种列式存储格式，非常高效
# 需要安装pyarrow或fastparquet: pip install pyarrow

# 创建数据
df = pd.DataFrame({
    'A': np.random.randn(1000),
    'B': np.random.randn(1000),
    'C': np.random.randn(1000)
})

print("Parquet文件操作示例：")
print(df.head())

# 写入Parquet
# df.to_parquet('data.parquet', engine='pyarrow', compression='snappy')

# 读取Parquet
# df_parquet = pd.read_parquet('data.parquet', engine='pyarrow')

# Parquet的优势：
# 1. 高效的列式存储
# 2. 支持复杂的数据类型
# 3. 压缩率高
# 4. 读取速度快
```

## 11.7 其他格式

### 11.7.1 Pickle格式

```python
# Pickle是Python的序列化格式
# 保留了DataFrame的所有信息（包括索引、数据类型等）

df = pd.DataFrame({
    'A': [1, 2, 3],
    'B': ['a', 'b', 'c']
}, index=['x', 'y', 'z'])

print("Pickle格式示例：")
print(df)

# 写入Pickle
# df.to_pickle('data.pkl')

# 读取Pickle
# df_pkl = pd.read_pickle('data.pkl')

# 注意：Pickle文件可能包含恶意代码，只加载信任的文件
```

### 11.7.2 HTML格式

```python
# 读取HTML表格
html_str = """
<table>
    <tr><th>Name</th><th>Age</th></tr>
    <tr><td>Alice</td><td>25</td></tr>
    <tr><td>Bob</td><td>30</td></tr>
</table>
"""

# 从HTML读取（返回DataFrame列表）
# dfs = pd.read_html(html_str)
# df = dfs[0]
print("HTML表格解析示例")

# 写入HTML
df = pd.DataFrame({
    'Name': ['Alice', 'Bob'],
    'Age': [25, 30]
})

html_output = df.to_html(index=False)
print("\nHTML输出：")
print(html_output)
```

### 11.7.3 剪贴板

```python
# 从剪贴板读取
# df = pd.read_clipboard()

# 写入剪贴板
df = pd.DataFrame({
    'A': [1, 2, 3],
    'B': [4, 5, 6]
})
# df.to_clipboard(index=False)
print("剪贴板操作示例：")
print(df)
```

## 11.8 大文件处理

### 11.8.1 分块读取

```python
# 创建大文件模拟数据
large_data = pd.DataFrame({
    'A': np.random.randn(1000),
    'B': np.random.randn(1000),
    'C': np.random.randn(1000)
})

# 模拟分块读取
csv_string = large_data.to_csv(index=False)

# 分块读取
chunk_size = 100
chunks = []

# 使用StringIO模拟文件读取
for chunk in pd.read_csv(StringIO(csv_string), chunksize=chunk_size):
    # 处理每个chunk
    processed_chunk = chunk[chunk['A'] > 0]
    chunks.append(processed_chunk)

# 合并结果
result = pd.concat(chunks, ignore_index=True)
print("分块读取结果：")
print(f"原始数据: {len(large_data)} 行")
print(f"处理后数据: {len(result)} 行")
print(result.head())
```

### 11.8.2 使用迭代器

```python
# 创建CSV迭代器
csv_string = large_data.to_csv(index=False)
reader = pd.read_csv(StringIO(csv_string), iterator=True)

# 读取指定行数
df_chunk = reader.get_chunk(50)
print("\n使用迭代器读取50行：")
print(df_chunk.head())
```

### 11.8.3 低内存模式

```python
# 对于大型CSV文件，可以使用low_memory选项
# df = pd.read_csv('large_file.csv', low_memory=False)

# 或者指定数据类型以减少内存使用
# dtypes = {'col1': 'int32', 'col2': 'float32'}
# df = pd.read_csv('large_file.csv', dtype=dtypes)

print("大文件处理建议：")
print("1. 使用分块读取")
print("2. 指定数据类型")
print("3. 只读取需要的列")
print("4. 使用更高效的格式（Parquet、HDF5）")
```

## 11.9 压缩文件

```python
# Pandas支持读写压缩文件
# 支持的格式：gzip, bz2, zip, xz

# 写入压缩CSV
df = pd.DataFrame({
    'A': range(1000),
    'B': np.random.randn(1000)
})

# df.to_csv('data.csv.gz', compression='gzip', index=False)
# df.to_csv('data.csv.bz2', compression='bz2', index=False)
# df.to_csv('data.csv.zip', compression='zip', index=False)

print("压缩文件操作示例")

# 读取压缩文件（自动检测）
# df = pd.read_csv('data.csv.gz')
# df = pd.read_csv('data.csv.zip')
```

## 11.10 综合案例：多格式数据处理

```python
# 创建示例数据
np.random.seed(42)
df = pd.DataFrame({
    'Date': pd.date_range('2024-01-01', periods=100),
    'Product': np.random.choice(['A', 'B', 'C'], 100),
    'Sales': np.random.randint(100, 1000, 100),
    'Quantity': np.random.randint(1, 20, 100)
})

print("原始数据：")
print(df.head())

# 1. 保存为CSV
csv_output = df.to_csv(index=False)
print("\n1. CSV格式（前200字符）：")
print(csv_output[:200])

# 2. 保存为JSON
json_output = df.head().to_json(orient='records', indent=2)
print("\n2. JSON格式（前5行）：")
print(json_output)

# 3. 保存到SQLite
conn = sqlite3.connect(':memory:')
df.to_sql('sales', conn, if_exists='replace', index=False)
df_sql = pd.read_sql('SELECT * FROM sales WHERE Sales > 500', conn)
print("\n3. SQL查询结果（前5行）：")
print(df_sql.head())
conn.close()

# 4. 数据摘要
print("\n4. 数据摘要：")
summary = df.groupby('Product').agg({
    'Sales': ['sum', 'mean', 'count'],
    'Quantity': ['sum', 'mean']
})
print(summary)

print("\n所有格式处理完成！")
```

## 11.11 性能比较

```python
import time

# 创建测试数据
n = 100000
df_large = pd.DataFrame({
    'A': np.random.randn(n),
    'B': np.random.randn(n),
    'C': np.random.randn(n),
    'D': np.random.choice(['X', 'Y', 'Z'], n)
})

print(f"测试数据: {n} 行 x {len(df_large.columns)} 列")

# CSV写入
start = time.time()
csv_str = df_large.to_csv(index=False)
csv_time = time.time() - start
print(f"\nCSV写入时间: {csv_time:.4f}秒")
print(f"CSV大小: {len(csv_str)} 字节")

# JSON写入
start = time.time()
json_str = df_large.to_json(orient='records')
json_time = time.time() - start
print(f"JSON写入时间: {json_time:.4f}秒")
print(f"JSON大小: {len(json_str)} 字节")

print("\n性能建议：")
print("- CSV: 通用格式，可读性好")
print("- JSON: 适合Web应用，结构化好")
print("- Parquet: 大数据处理的首选")
print("- HDF5: 科学计算的首选")
print("- Pickle: Python特定，速度快")
```

## 11.12 练习题

1. **基础练习**：创建一个DataFrame，分别保存为CSV、JSON和Excel格式。

2. **数据转换**：读取CSV文件，进行数据清洗，然后保存为多种格式。

3. **大文件处理**：创建一个大型CSV文件，使用分块方式处理。

4. **数据库操作**：创建SQLite数据库，进行增删改查操作。

5. **综合案例**：从多个不同格式的文件中读取数据，合并处理后输出。

## 11.13 小结

本章介绍了Pandas中的文件读写操作：

- **CSV文件**：最常用的文本格式，read_csv/to_csv
- **Excel文件**：商业应用常用，read_excel/to_excel
- **JSON文件**：Web应用常用，read_json/to_json
- **SQL数据库**：关系型数据库，read_sql/to_sql
- **其他格式**：HDF5、Parquet、Pickle、HTML等
- **大文件处理**：分块读取、迭代器、低内存模式
- **压缩文件**：支持多种压缩格式

选择合适的文件格式取决于：
- 数据量大小
- 读写速度要求
- 跨平台兼容性
- 人类可读性

## 下一章

在下一章中，我们将学习**数据类型和转换**，包括类别数据、日期时间类型等。
