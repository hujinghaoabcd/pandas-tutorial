# 第17章：性能优化

性能优化对于处理大数据集至关重要。本章将介绍Pandas性能优化的各种技巧和最佳实践。

## 17.1 向量化操作

```python
import pandas as pd
import numpy as np
import time

# 创建大数据集
n = 1000000
df = pd.DataFrame({
    'A': np.random.randn(n),
    'B': np.random.randn(n)
})

# 方法1：循环（慢）
start = time.time()
result_loop = []
for i in range(len(df)):
    result_loop.append(df.iloc[i]['A'] + df.iloc[i]['B'])
time_loop = time.time() - start

# 方法2：向量化（快）
start = time.time()
result_vectorized = df['A'] + df['B']
time_vectorized = time.time() - start

print(f"循环方法耗时: {time_loop:.4f}秒")
print(f"向量化方法耗时: {time_vectorized:.4f}秒")
print(f"加速比: {time_loop/time_vectorized:.2f}x")
```

## 17.2 使用合适的数据类型

```python
# 创建数据
df = pd.DataFrame({
    'int_col': range(100000),
    'category_col': np.random.choice(['A', 'B', 'C'], 100000)
})

print("优化前：")
print(df.dtypes)
print(f"内存: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

# 优化数据类型
df_optimized = df.copy()
df_optimized['int_col'] = df_optimized['int_col'].astype('int32')
df_optimized['category_col'] = df_optimized['category_col'].astype('category')

print("\n优化后：")
print(df_optimized.dtypes)
print(f"内存: {df_optimized.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

reduction = (1 - df_optimized.memory_usage(deep=True).sum() / df.memory_usage(deep=True).sum()) * 100
print(f"\n内存减少: {reduction:.1f}%")
```

## 17.3 避免不必要的复制

```python
# 创建数据
df = pd.DataFrame({'A': range(1000000)})

# 方法1：创建副本（慢）
start = time.time()
df_copy = df.copy()
df_copy['B'] = df_copy['A'] * 2
time_copy = time.time() - start

# 方法2：就地操作（快）
df_inplace = df.copy()  # 为了公平比较
start = time.time()
df_inplace['B'] = df_inplace['A'] * 2
time_inplace = time.time() - start

print(f"复制方法耗时: {time_copy:.4f}秒")
print(f"就地方法耗时: {time_inplace:.4f}秒")
```

## 17.4 使用分块处理

```python
# 模拟大文件处理
chunk_size = 10000
chunks = []

# 创建大数据集并分块处理
for i in range(10):
    chunk = pd.DataFrame({
        'A': np.random.randn(chunk_size),
        'B': np.random.randn(chunk_size)
    })
    # 对每个chunk进行处理
    chunk_processed = chunk[chunk['A'] > 0]
    chunks.append(chunk_processed)

# 合并结果
result = pd.concat(chunks, ignore_index=True)
print(f"处理了 {len(result)} 行数据")
```

## 17.5 使用NumPy函数

```python
# 创建数据
df = pd.DataFrame({
    'A': np.random.randn(100000),
    'B': np.random.randn(100000)
})

# 方法1：Pandas方法
start = time.time()
result1 = df['A'].apply(lambda x: np.sqrt(x**2 + 1))
time_pandas = time.time() - start

# 方法2：NumPy函数
start = time.time()
result2 = np.sqrt(df['A'].values**2 + 1)
time_numpy = time.time() - start

print(f"Pandas方法耗时: {time_pandas:.4f}秒")
print(f"NumPy方法耗时: {time_numpy:.4f}秒")
print(f"加速比: {time_pandas/time_numpy:.2f}x")
```

## 17.6 索引优化

```python
# 创建数据
df = pd.DataFrame({
    'Key': np.random.choice(['A', 'B', 'C'], 100000),
    'Value': np.random.randn(100000)
})

# 不使用索引
start = time.time()
for key in ['A', 'B', 'C']:
    subset = df[df['Key'] == key]
    result = subset['Value'].mean()
time_no_index = time.time() - start

# 使用索引
df_indexed = df.set_index('Key')
start = time.time()
for key in ['A', 'B', 'C']:
    result = df_indexed.loc[key, 'Value'].mean()
time_with_index = time.time() - start

print(f"无索引耗时: {time_no_index:.4f}秒")
print(f"有索引耗时: {time_with_index:.4f}秒")
print(f"加速比: {time_no_index/time_with_index:.2f}x")
```

## 17.7 并行处理

```python
# 使用multiprocessing进行并行处理
from multiprocessing import Pool
import pandas as pd

def process_chunk(chunk):
    # 处理每个数据块
    return chunk[chunk['A'] > 0].shape[0]

# 创建数据
df = pd.DataFrame({'A': np.random.randn(1000000)})

# 分割数据
chunks = np.array_split(df, 4)

# 并行处理
with Pool(4) as pool:
    results = pool.map(process_chunk, chunks)

print(f"总共有 {sum(results)} 个正数")
```

## 17.8 性能分析

```python
# 使用profile进行性能分析
def slow_function(df):
    result = []
    for i in range(len(df)):
        result.append(df.iloc[i]['A'] * 2)
    return result

def fast_function(df):
    return df['A'] * 2

df = pd.DataFrame({'A': range(10000)})

# 比较性能
import cProfile
import pstats
from io import StringIO

pr = cProfile.Profile()
pr.enable()
slow_function(df)
pr.disable()

s = StringIO()
ps = pstats.Stats(pr, stream=s).sort_stats('cumulative')
ps.print_stats(10)
print("慢速函数性能分析：")
print(s.getvalue()[:500])  # 只显示前500字符

print("\n快速函数性能分析：")
%timeit fast_function(df)  # 如果在Jupyter中运行
```

## 17.9 最佳实践总结

```python
# 性能优化清单
performance_tips = """
Pandas性能优化清单：

1. 使用向量化操作代替循环
2. 选择合适的数据类型（category, int32等）
3. 避免不必要的数据复制
4. 使用分块处理大文件
5. 优先使用NumPy函数
6. 为频繁查询的列建立索引
7. 使用inplace参数（谨慎使用）
8. 避免链式索引
9. 使用query()代替复杂的布尔索引
10. 考虑使用Dask处理超大数据集
"""

print(performance_tips)
```
