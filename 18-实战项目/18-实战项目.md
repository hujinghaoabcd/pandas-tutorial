# ç¬¬18ç« ï¼šå®æˆ˜é¡¹ç›®

æœ¬ç« å°†é€šè¿‡å‡ ä¸ªå®Œæ•´çš„å®æˆ˜é¡¹ç›®ï¼Œç»¼åˆè¿ç”¨å‰é¢å­¦åˆ°çš„æ‰€æœ‰PandasæŠ€æœ¯ã€‚

## 18.1 é¡¹ç›®ä¸€ï¼šé”€å”®æ•°æ®åˆ†æ

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# åˆ›å»ºæ¨¡æ‹Ÿçš„é”€å”®æ•°æ®
np.random.seed(42)
dates = pd.date_range('2023-01-01', '2023-12-31', freq='D')
n_products = 5
n_regions = 4

data = []
for date in dates:
    for product in range(1, n_products + 1):
        for region in ['North', 'South', 'East', 'West']:
            data.append({
                'Date': date,
                'Product': f'Product{product}',
                'Region': region,
                'Sales': np.random.randint(100, 1000),
                'Quantity': np.random.randint(10, 100),
                'Cost': np.random.randint(50, 500)
            })

df = pd.DataFrame(data)
print("æ•°æ®æ¦‚è§ˆï¼š")
print(df.head(10))
print(f"\næ•°æ®å½¢çŠ¶: {df.shape}")

# 1. æ•°æ®æ¸…æ´—
print("\n1. æ•°æ®æ¸…æ´—ï¼š")
print(f"ç¼ºå¤±å€¼: {df.isnull().sum().sum()}")
print(f"é‡å¤å€¼: {df.duplicated().sum()}")

# 2. æ·»åŠ è®¡ç®—åˆ—
df['Profit'] = df['Sales'] - df['Cost']
df['ProfitMargin'] = (df['Profit'] / df['Sales'] * 100).round(2)
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Quarter'] = df['Date'].dt.quarter

# 3. é”€å”®è¶‹åŠ¿åˆ†æ
print("\n2. é”€å”®è¶‹åŠ¿åˆ†æï¼š")
monthly_sales = df.groupby('Month')['Sales'].sum()
print(monthly_sales)

# 4. äº§å“åˆ†æ
print("\n3. äº§å“é”€å”®æ’åï¼š")
product_sales = df.groupby('Product').agg({
    'Sales': 'sum',
    'Quantity': 'sum',
    'Profit': 'sum'
}).sort_values('Sales', ascending=False)
print(product_sales)

# 5. åœ°åŒºåˆ†æ
print("\n4. åœ°åŒºé”€å”®åˆ†æï¼š")
region_analysis = df.groupby('Region').agg({
    'Sales': ['sum', 'mean'],
    'Profit': ['sum', 'mean'],
    'Quantity': 'sum'
})
print(region_analysis)

# 6. æ—¶é—´åºåˆ—åˆ†æ
print("\n5. å­£åº¦å¯¹æ¯”ï¼š")
quarterly_sales = df.groupby(['Quarter', 'Product'])['Sales'].sum().unstack()
print(quarterly_sales)

# 7. åˆ©æ¶¦ç‡åˆ†æ
print("\n6. å¹³å‡åˆ©æ¶¦ç‡æœ€é«˜çš„äº§å“ï¼š")
profit_margin = df.groupby('Product')['ProfitMargin'].mean().sort_values(ascending=False)
print(profit_margin)

# 8. åŒæ¯”åˆ†æï¼ˆå¦‚æœæœ‰å¤šå¹´æ•°æ®ï¼‰
print("\n7. æœˆåº¦ç¯æ¯”å¢é•¿ï¼š")
monthly_total = df.groupby('Month')['Sales'].sum()
mom_growth = monthly_total.pct_change() * 100
print(mom_growth.round(2))

print("\n\n=== é¡¹ç›®ä¸€å®Œæˆ ===\n")
```

## 18.2 é¡¹ç›®äºŒï¼šç”¨æˆ·è¡Œä¸ºåˆ†æ

```python
# åˆ›å»ºç”¨æˆ·è¡Œä¸ºæ•°æ®
np.random.seed(42)
n_users = 1000
n_days = 90

user_data = []
for user_id in range(1, n_users + 1):
    for day in range(n_days):
        date = pd.Timestamp('2024-01-01') + pd.Timedelta(days=day)
        # æ¨¡æ‹Ÿç”¨æˆ·æ´»è·ƒåº¦ï¼ˆå‘¨æœ«æ´»è·ƒåº¦æ›´é«˜ï¼‰
        is_weekend = date.dayofweek >= 5
        active_prob = 0.3 if not is_weekend else 0.5

        if np.random.random() < active_prob:
            user_data.append({
                'UserId': f'U{user_id:04d}',
                'Date': date,
                'PageViews': np.random.randint(1, 50),
                'TimeSpent': np.random.randint(1, 120),  # åˆ†é’Ÿ
                'Actions': np.random.randint(0, 10)
            })

df_users = pd.DataFrame(user_data)
print("ç”¨æˆ·è¡Œä¸ºæ•°æ®ï¼š")
print(df_users.head(10))
print(f"\næ•°æ®å½¢çŠ¶: {df_users.shape}")

# 1. ç”¨æˆ·æ´»è·ƒåº¦åˆ†æ
print("\n1. æ—¥æ´»è·ƒç”¨æˆ·ï¼ˆDAUï¼‰ï¼š")
dau = df_users.groupby('Date')['UserId'].nunique()
print(f"å¹³å‡DAU: {dau.mean():.0f}")
print(f"æœ€é«˜DAU: {dau.max()}")
print(f"æœ€ä½DAU: {dau.min()}")

# 2. ç”¨æˆ·ç•™å­˜åˆ†æ
print("\n2. ç”¨æˆ·ç•™å­˜ç‡ï¼š")
first_date = df_users.groupby('UserId')['Date'].min()
df_users['DaysSinceFirst'] = (df_users['Date'] - df_users['UserId'].map(first_date)).dt.days

retention = df_users.groupby('DaysSinceFirst')['UserId'].nunique()
retention_rate = (retention / retention.iloc[0] * 100).round(2)
print("Day 1ç•™å­˜:", retention_rate.get(1, 0), "%")
print("Day 7ç•™å­˜:", retention_rate.get(7, 0), "%")
print("Day 30ç•™å­˜:", retention_rate.get(30, 0), "%")

# 3. ç”¨æˆ·åˆ†å±‚
print("\n3. ç”¨æˆ·æ´»è·ƒåº¦åˆ†å±‚ï¼š")
user_activity = df_users.groupby('UserId').agg({
    'Date': 'count',
    'PageViews': 'sum',
    'Actions': 'sum'
}).rename(columns={'Date': 'ActiveDays'})

def classify_user(row):
    if row['ActiveDays'] >= 30:
        return 'High'
    elif row['ActiveDays'] >= 10:
        return 'Medium'
    else:
        return 'Low'

user_activity['ActivityLevel'] = user_activity.apply(classify_user, axis=1)
print(user_activity['ActivityLevel'].value_counts())

# 4. RFMåˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
print("\n4. ç®€åŒ–RFMåˆ†æï¼š")
last_date = df_users['Date'].max()
rfm = df_users.groupby('UserId').agg({
    'Date': lambda x: (last_date - x.max()).days,  # Recency
    'UserId': 'count',  # Frequency
    'PageViews': 'sum'  # Monetary (ç”¨PageViewsä»£æ›¿)
})
rfm.columns = ['Recency', 'Frequency', 'Monetary']
print(rfm.describe())

print("\n\n=== é¡¹ç›®äºŒå®Œæˆ ===\n")
```

## 18.3 é¡¹ç›®ä¸‰ï¼šæ•°æ®è´¨é‡æ£€æŸ¥æŠ¥å‘Š

```python
def generate_data_quality_report(df):
    """ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š"""

    report = []

    # 1. åŸºæœ¬ä¿¡æ¯
    report.append("=" * 50)
    report.append("æ•°æ®è´¨é‡æŠ¥å‘Š")
    report.append("=" * 50)
    report.append(f"\næ•°æ®å½¢çŠ¶: {df.shape[0]} è¡Œ x {df.shape[1]} åˆ—")

    # 2. ç¼ºå¤±å€¼åˆ†æ
    report.append("\n\nç¼ºå¤±å€¼åˆ†æ:")
    report.append("-" * 50)
    missing = df.isnull().sum()
    missing_pct = (missing / len(df) * 100).round(2)
    missing_df = pd.DataFrame({
        'ç¼ºå¤±æ•°é‡': missing,
        'ç¼ºå¤±æ¯”ä¾‹(%)': missing_pct
    })
    missing_df = missing_df[missing_df['ç¼ºå¤±æ•°é‡'] > 0].sort_values('ç¼ºå¤±æ•°é‡', ascending=False)
    if len(missing_df) > 0:
        report.append(missing_df.to_string())
    else:
        report.append("æ— ç¼ºå¤±å€¼")

    # 3. é‡å¤å€¼åˆ†æ
    report.append("\n\né‡å¤å€¼åˆ†æ:")
    report.append("-" * 50)
    duplicates = df.duplicated().sum()
    report.append(f"é‡å¤è¡Œæ•°: {duplicates} ({duplicates/len(df)*100:.2f}%)")

    # 4. æ•°æ®ç±»å‹
    report.append("\n\næ•°æ®ç±»å‹:")
    report.append("-" * 50)
    report.append(df.dtypes.to_string())

    # 5. æ•°å€¼åˆ—ç»Ÿè®¡
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        report.append("\n\næ•°å€¼åˆ—ç»Ÿè®¡:")
        report.append("-" * 50)
        report.append(df[numeric_cols].describe().to_string())

    # 6. ç±»åˆ«åˆ—ç»Ÿè®¡
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns
    if len(categorical_cols) > 0:
        report.append("\n\nç±»åˆ«åˆ—å”¯ä¸€å€¼æ•°é‡:")
        report.append("-" * 50)
        for col in categorical_cols:
            unique_count = df[col].nunique()
            report.append(f"{col}: {unique_count}")

    # 7. å†…å­˜ä½¿ç”¨
    report.append("\n\nå†…å­˜ä½¿ç”¨:")
    report.append("-" * 50)
    memory_usage = df.memory_usage(deep=True).sum() / 1024**2
    report.append(f"æ€»å†…å­˜: {memory_usage:.2f} MB")

    return "\n".join(report)

# æµ‹è¯•æ•°æ®è´¨é‡æŠ¥å‘Š
test_df = pd.DataFrame({
    'A': [1, 2, np.nan, 4, 5, 5],
    'B': ['x', 'y', 'z', 'x', 'y', 'y'],
    'C': pd.date_range('2024-01-01', periods=6),
    'D': [1.1, 2.2, 3.3, np.nan, 5.5, 6.6]
})

print(generate_data_quality_report(test_df))

print("\n\n=== é¡¹ç›®ä¸‰å®Œæˆ ===\n")
```

## 18.4 é¡¹ç›®å››ï¼šA/Bæµ‹è¯•åˆ†æ

```python
# A/Bæµ‹è¯•æ•°æ®
np.random.seed(42)
n_users_per_group = 1000

# å¯¹ç…§ç»„ï¼ˆAï¼‰
group_a = pd.DataFrame({
    'UserId': [f'A{i:04d}' for i in range(n_users_per_group)],
    'Group': 'A',
    'Converted': np.random.binomial(1, 0.10, n_users_per_group),  # 10%è½¬åŒ–ç‡
    'Revenue': np.random.gamma(2, 10, n_users_per_group)
})

# å®éªŒç»„ï¼ˆBï¼‰
group_b = pd.DataFrame({
    'UserId': [f'B{i:04d}' for i in range(n_users_per_group)],
    'Group': 'B',
    'Converted': np.random.binomial(1, 0.12, n_users_per_group),  # 12%è½¬åŒ–ç‡
    'Revenue': np.random.gamma(2, 12, n_users_per_group)
})

ab_test = pd.concat([group_a, group_b], ignore_index=True)
print("A/Bæµ‹è¯•æ•°æ®ï¼š")
print(ab_test.head(10))

# åˆ†æ
print("\nè½¬åŒ–ç‡åˆ†æï¼š")
conversion = ab_test.groupby('Group')['Converted'].agg(['sum', 'count', 'mean'])
conversion['ConversionRate'] = (conversion['mean'] * 100).round(2)
print(conversion[['sum', 'count', 'ConversionRate']])

print("\næ”¶å…¥åˆ†æï¼š")
revenue = ab_test.groupby('Group')['Revenue'].agg(['sum', 'mean', 'median'])
print(revenue.round(2))

# ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒï¼ˆç®€å•ç‰ˆï¼‰
from scipy import stats
group_a_converted = group_a['Converted']
group_b_converted = group_b['Converted']
_, p_value = stats.ttest_ind(group_a_converted, group_b_converted)
print(f"\nP-value: {p_value:.4f}")
if p_value < 0.05:
    print("ç»“æœå…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§ (p < 0.05)")
else:
    print("ç»“æœä¸å…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§ (p >= 0.05)")

print("\n\n=== é¡¹ç›®å››å®Œæˆ ===\n")
```

## 18.5 æ€»ç»“

```python
print("""
å®æˆ˜é¡¹ç›®æ€»ç»“ï¼š

1. é”€å”®æ•°æ®åˆ†æï¼š
   - æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†
   - å¤šç»´åº¦åˆ†æï¼ˆæ—¶é—´ã€äº§å“ã€åœ°åŒºï¼‰
   - è¶‹åŠ¿åˆ†æå’ŒåŒæ¯”åˆ†æ

2. ç”¨æˆ·è¡Œä¸ºåˆ†æï¼š
   - æ´»è·ƒåº¦æŒ‡æ ‡ï¼ˆDAUã€MAUï¼‰
   - ç•™å­˜ç‡è®¡ç®—
   - ç”¨æˆ·åˆ†å±‚ï¼ˆRFMï¼‰

3. æ•°æ®è´¨é‡æ£€æŸ¥ï¼š
   - è‡ªåŠ¨åŒ–è´¨é‡æŠ¥å‘Š
   - ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼æ£€æµ‹
   - æ•°æ®ç±»å‹éªŒè¯

4. A/Bæµ‹è¯•åˆ†æï¼š
   - å¯¹ç…§ç»„vså®éªŒç»„å¯¹æ¯”
   - è½¬åŒ–ç‡åˆ†æ
   - ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ

è¿™äº›é¡¹ç›®æ¶µç›–äº†æ•°æ®åˆ†æçš„å¸¸è§åœºæ™¯ï¼Œå¯ä»¥ä½œä¸ºå®é™…å·¥ä½œçš„æ¨¡æ¿ã€‚
""")
```

## 18.6 Pandaså­¦ä¹ è·¯çº¿æ€»ç»“

```python
learning_path = """
Pandaså­¦ä¹ è·¯çº¿å›¾ï¼š

ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€ï¼ˆç¬¬1-4ç« ï¼‰
âœ“ Pandasç®€ä»‹å’Œå®‰è£…
âœ“ Serieså’ŒDataFrame
âœ“ æ•°æ®é€‰æ‹©å’Œç´¢å¼•

ç¬¬äºŒé˜¶æ®µï¼šæ•°æ®å¤„ç†ï¼ˆç¬¬5-8ç« ï¼‰
âœ“ æ•°æ®æ¸…æ´—
âœ“ æ•°æ®åˆå¹¶å’Œè¿æ¥
âœ“ æ•°æ®åˆ†ç»„å’Œèšåˆ
âœ“ æ•°æ®é‡å¡‘å’Œé€è§†

ç¬¬ä¸‰é˜¶æ®µï¼šä¸“é¢˜ï¼ˆç¬¬9-12ç« ï¼‰
âœ“ æ—¶é—´åºåˆ—
âœ“ æ•°æ®å¯è§†åŒ–
âœ“ æ–‡ä»¶è¯»å†™
âœ“ æ•°æ®ç±»å‹å’Œè½¬æ¢

ç¬¬å››é˜¶æ®µï¼šè¿›é˜¶ï¼ˆç¬¬13-17ç« ï¼‰
âœ“ ç¼ºå¤±å€¼å¤„ç†
âœ“ æ•°æ®æ’åºå’Œæ’å
âœ“ å­—ç¬¦ä¸²æ“ä½œ
âœ“ é«˜çº§æŠ€å·§
âœ“ æ€§èƒ½ä¼˜åŒ–

ç¬¬äº”é˜¶æ®µï¼šå®æˆ˜ï¼ˆç¬¬18ç« ï¼‰
âœ“ å®Œæ•´é¡¹ç›®å®è·µ

æ­å–œä½ å®ŒæˆPandasä»å…¥é—¨åˆ°ç²¾é€šæ•™ç¨‹ï¼ğŸ‰
"""

print(learning_path)
```

å®Œæ•´æ•™ç¨‹ç»“æŸï¼
